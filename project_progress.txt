老师，我这几天把远东学长的爬虫跑起来了，添加了一些其他功能。
第1、2点是远东学长希望我实现的，让这个爬虫更通用

1、所有的爬取规则从配置文件读取(当然也能从其他地方，例如从数据库读取)，增加通用性，这样的话这个爬虫可以作为一个通用的模板，规则自定义添加，不用每次改代码
2、数据SlavenodeDataItem类能够根据配置文件动态创建，并且继承于scrapy.Item()类，这样字段可以更灵活，不用写死在代码里
2、添加了BeautifulSoup4的支持，在爬取规则的定义文件里可以选择用xpath还是BS4
3、BS4现在可以选择使用lxml引擎了，速度比以往的BS4默认引擎要快很多，而且应该不会比xpath慢。而且BS4比xpath功能更强大，使用更方便
4、MongoDB连接的部分支持SSL连接MongoDB的副本集，由于我之前已经搭好了现成的MongoDB Docker集群，所以添加了这个支持，并且做好了读写分离
5、所有爬虫发出的请求可以全部通过Tor代理，实现随机IP，防止被Ban(不过这个会导致爬取速度显著变慢)，可以选择是否使用


我测试了爬Stack Overflow，一开始挺正常，不过之后因为爬取速度太快，Response报了429错误，原因是请求速度太快被ban了...
所以我找了两个解决办法：
1、流量全部通过Tor，随机IP，这样速度太慢，不考虑
2、我调高了DOWNLOAD_DELAY，从0.3s提高到了0.9s，也就是说平常没被ban的时候发包随机间隔0.45s~1.35s
并且我再写了一个Scrapy中间件，判断如果HTTP返回值429(真被ban了)，那么就等待1~60s的随机秒数，然后重新发出这个被ban的request，防止漏爬被ban的request
也就是说，平时正常包等待少量时间，如果被ban就等待很长的间隔，等恢复后再把被ban的request重新加入Scheduler发包队列
第二个办法很好用，Stack Overflow也爬的差不多了，一切正常(只爬了几十万条，不敢爬太多，怕MongoDB内存不够....)

我接下来想做的是：
1、把爬虫放到Docker Swarm集群上，多节点分布式统一管理(快做好了)
2、与之前的前端部分做好衔接，让爬取规则的定义模板部分从前端的网页部分获取(或者是前端将爬取规则定义文件放到数据库里，然后爬虫从数据库里读取规则模板，)
3、增加模拟论坛登陆模块(貌似爬虫组有现成的登录模块，不知道能不能很好的整合在一起)
4、希望增加随机代理池，每个包通过不同的代理IP(网上免费的普通代理好像比较难找，这里想整合shadowsocks的功能，把数据包随机发到各个ss服务器来转发，感觉有点难实现)


DATA_PARSE_RULES = {
    'mode': 'BeautifulSoup',
    'rules': {
        'title': {
            'to_str': False,
            'raw_expr': False,
            'eval': "find(name='a', attrs={'class': 'question-hyperlink'}).string.strip()"
        },
        'content': {
            'to_str': True,
            'raw_expr': False,
            'eval': "find(name='div', attrs={'class': 'post-text', 'itemprop': 'text'})"
        },
        'url': {
            'to_str': False,
            'raw_expr': True,
            'eval': "response.url"
        },
        'forum': {
            'to_str': False,
            'raw_expr': True,
            'eval': "urllib.splithost(urllib.splittype(response.url)[1])[0]"
        },
        'author': {
            'to_str': False,
            'raw_expr': False,
            'eval': "find(name='div', attrs={'class': 'user-details'}).a.string.strip()"
        },
        'datetime': {
            'to_str': False,
            'raw_expr': True,
            'eval': "datetime.strptime(soup.find(name='div',attrs={'class': ['module', 'question-stats']}).find_all(name='p',attrs={'class': 'label-key'})[1]['title'].strip()[:-1], '%Y-%m-%d %H:%M:%S')"
        },
        'question_id': {
            'to_str': False,
            'raw_expr': True,
            'eval': "int(re.search('(?<=(questions))/(\d+)/',response.url).group(2))"
        },
    }
}

URL_PARSE_RULES = {
    'mode': 'xpath',
    'link_extractor': {
        'restrict_xpaths': '//div[@class="pager fl"]/a[@rel="next"]',
        # 'process_value': 'lambda v: "https://stackoverflow.com" + v',
        # 'allow': r'^/questions/\d+/(?:[a-zA-Z\d]+-)+(?:[a-zA-Z\d]+)$',
        'allow': r'^https://stackoverflow.com/questions',
    },
    'rules': '//a[@class="question-hyperlink"]/@href',
    'process_value': 'lambda v: "https://stackoverflow.com" + v'
}

配置文件大概长这样，这两个dict可以从任何地方读取，不一定写到文件中
而且读取规则每个爬虫只会被初始化一次，不会不停的读取规则浪费时间
我看到学长代码里用了Bloom Filter查重算法，很不错....但是参数好像可以再调优